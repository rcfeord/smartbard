{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../raw_data/limerick_dataset_oedilf_v3.json'\n",
    "df = pd.read_json(file_path)\n",
    "df = df[df.is_limerick == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = df['limerick'][8]\n",
    "lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_tk = \"<LS> <KS> tomatoes <KE> Our tomatoes this year are abounding; <L0> They're lush, red and ripe — just astounding! <L1> We've run out of uses <L2> (We're making excuses) <L3> For produce that's all but dumbfounding. <L4> <LE>\"\n",
    "lim_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LS> <KS> tomatoes <KE> Our tomatoes this year are [MASK]; <L0> They're lush, red and ripe — just [MASK]! <L1> We've run out of [MASK] <L2> (We're making [MASK]) <L3> For produce that's all but [MASK]. <L4> <LE>\n",
      "{'masked': [(0, 'abounding'), (1, 'astounding'), (2, 'uses'), (3, 'excuses'), (4, 'dumbfounding')], 'unmasked': []}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Our tomatoes this year are [MASK]; They're lush, red and ripe — just [MASK]! We've run out of [MASK] (We're making [MASK]) For produce that's all but [MASK].\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_processing.bert_preproc import zorro\n",
    "\n",
    "masked_lim, unmasked = zorro(lim_tk, 0, 1, 2, 3, 4) \n",
    "print(masked_lim)\n",
    "print(unmasked)\n",
    "masked_lim = masked_lim[masked_lim.find('<KE>'):]\n",
    "masked_lim = masked_lim.replace('<KE> ','')\n",
    "masked_lim = masked_lim.replace('<L0> ','')\n",
    "masked_lim = masked_lim.replace('<L1> ','')\n",
    "masked_lim = masked_lim.replace('<L2> ','')\n",
    "masked_lim = masked_lim.replace('<L3> ','')\n",
    "masked_lim = masked_lim.replace(' <L4> <LE>','')\n",
    "masked_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhyme_word_pred(masked_lim: str, top_k=50) -> tuple:\n",
    "    \"\"\"\n",
    "    takes a limerick with masked words\n",
    "    uses BERT to predicted possible replacement words in mask location\n",
    "    \"\"\"\n",
    "    tz = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "    model.eval()\n",
    "    \n",
    "    tokenized_text = tz.tokenize(masked_lim)\n",
    "    indexed_tokens = tz.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    masked_pos = [i for i,d in enumerate(tokenized_text) if d=='[MASK]']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    rhyme_list = ()\n",
    "    for msk_ind in masked_pos:\n",
    "        probs = torch.nn.functional.softmax(predictions[0, msk_ind], dim=-1)\n",
    "        top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "        \n",
    "        temp_list = []\n",
    "        for i, pred_idx in enumerate(top_k_indices):\n",
    "            \n",
    "            predicted_token = tz.convert_ids_to_tokens([pred_idx])[0]\n",
    "            token_weight = top_k_weights[i]\n",
    "            temp_tpl = (predicted_token, float(token_weight))\n",
    "        \n",
    "            temp_list.append(temp_tpl)\n",
    "\n",
    "    rhyme_list = rhyme_list + (temp_list,)\n",
    "    \n",
    "    return rhyme_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smartbard.bert_model.bert_predict_masked_words import rhyme_word_pred\n",
    "\n",
    "rhyme_list = rhyme_word_pred(masked_lim)\n",
    "rhyme_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tz.tokenize(masked_lim)\n",
    "indexed_tokens = tz.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tz.encode(masked_lim, return_tensors='pt')\n",
    "# masked_index = tokenized_text.index(\"[MASK]\")\n",
    "masked_pos = [i for i,d in enumerate(tokenized_text) if d=='[MASK]']\n",
    "# masked_position = (token_ids.squeeze() == tz.mask_token_id).nonzero()\n",
    "# masked_pos = [mask.item() for mask in masked_position ]\n",
    "# masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('amazing', 0.03770241141319275),\n",
       "  ('good', 0.030399274080991745),\n",
       "  ('great', 0.024686096236109734),\n",
       "  ('huge', 0.02320134826004505),\n",
       "  ('beautiful', 0.022430414333939552),\n",
       "  ('wonderful', 0.021738242357969284),\n",
       "  ('incredible', 0.01915503665804863),\n",
       "  ('fantastic', 0.018468396738171577),\n",
       "  ('rare', 0.018459055572748184),\n",
       "  ('perfect', 0.01667996123433113)],\n",
       " [('!', 0.12691663205623627),\n",
       "  ('look', 0.04611518234014511),\n",
       "  ('right', 0.040643367916345596),\n",
       "  ('stop', 0.03317881003022194),\n",
       "  ('wow', 0.0330524668097496),\n",
       "  ('think', 0.02918442152440548),\n",
       "  ('go', 0.022315895184874535),\n",
       "  ('see', 0.01810493879020214),\n",
       "  ('what', 0.017087459564208984),\n",
       "  ('no', 0.015778031200170517)],\n",
       " [('time', 0.08695467561483383),\n",
       "  ('options', 0.06752683222293854),\n",
       "  ('money', 0.042667098343372345),\n",
       "  ('space', 0.02524210326373577),\n",
       "  ('produce', 0.019716963171958923),\n",
       "  ('ingredients', 0.018899541348218918),\n",
       "  ('supplies', 0.017176149412989616),\n",
       "  ('food', 0.017155611887574196),\n",
       "  ('orders', 0.015196624211966991),\n",
       "  ('resources', 0.013337608426809311)],\n",
       " [('it', 0.204401895403862),\n",
       "  ('up', 0.0979369655251503),\n",
       "  ('them', 0.050995245575904846),\n",
       "  ('money', 0.0343206524848938),\n",
       "  ('wine', 0.020422935485839844),\n",
       "  ('bread', 0.012674655765295029),\n",
       "  ('millions', 0.01026904210448265),\n",
       "  ('out', 0.010220552794635296),\n",
       "  ('mistakes', 0.010219953022897243),\n",
       "  ('plenty', 0.008361687883734703)],\n",
       " [('.', 0.11149030178785324),\n",
       "  ('impossible', 0.02732507884502411),\n",
       "  ('new', 0.01605750434100628),\n",
       "  ('nothing', 0.015427771024405956),\n",
       "  ('gone', 0.01227277796715498),\n",
       "  ('worthless', 0.012176969088613987),\n",
       "  ('useless', 0.011390820145606995),\n",
       "  ('natural', 0.01091129519045353),\n",
       "  ('a', 0.010606289841234684),\n",
       "  ('dry', 0.009971310384571552)])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "top_k = 10\n",
    "words_and_proba = ()\n",
    "\n",
    "# msk_ind = masked_pos[0]\n",
    "# print(msk_ind)\n",
    "# probs = torch.nn.functional.softmax(predictions[0, msk_ind], dim=-1)\n",
    "# top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "# # print(masked_index)\n",
    "\n",
    "list_temp = []\n",
    "for i, pred_idx in enumerate(top_k_indices):\n",
    "    predicted_token = tz.convert_ids_to_tokens([pred_idx])[0]\n",
    "    token_weight = top_k_weights[i]\n",
    "    # print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "    list_temp.append((predicted_token, float(token_weight)))\n",
    "\n",
    "list_temp\n",
    "\n",
    "for msk_ind in masked_pos:\n",
    "    probs = torch.nn.functional.softmax(predictions[0, msk_ind], dim=-1)\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "    \n",
    "    temp_list = []\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        \n",
    "        predicted_token = tz.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        # print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "        temp_tpl = (predicted_token, float(token_weight))\n",
    "    \n",
    "        temp_list.append(temp_tpl)\n",
    "\n",
    "    words_and_proba = words_and_proba + (temp_list,)\n",
    "words_and_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Our tomatoes this year are abounding;\\nThey're lush, red and ripe — just astounding!\\nWe've run out of uses\\n(We're making excuses)\\nFor produce that's all but dumbfounding.\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pred_idx in enumerate(top_k_indices):\n",
    "    predicted_token = tz.convert_ids_to_tokens([pred_idx])[0]\n",
    "    # print(predicted_token)\n",
    "    token_weight = top_k_weights[i]\n",
    "    print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded['input_ids']\n",
    "attn_mask = encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tz = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In a cozy, [MASK] alcove I knit.\\nIt's my hole-in-the-wall, poorly lit.\\nI drop many a stitch,\\nYet my goal's to enrich\\nMy dear husband, whom this thing may fit.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = lim.replace(\"warm\", \"[MASK]\")\n",
    "# # text = \"[CLS] %s [SEP]\"%text\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tz.tokenize(text)\n",
    "masked_index = tokenized_text.index(\"[MASK]\")\n",
    "indexed_tokens = tz.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]: 'a'  | weights: 0.11124002188444138\n",
      "[MASK]: 'an'  | weights: 0.05427626520395279\n",
      "[MASK]: 'the'  | weights: 0.03255700320005417\n",
      "[MASK]: 'my'  | weights: 0.03207283094525337\n",
      "[MASK]: 'small'  | weights: 0.012609540484845638\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#         outputs = model(tokens_tensor)\n",
    "#         predictions = outputs[0]\n",
    "\n",
    "# probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "# top_k = 5\n",
    "# top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "# for i, pred_idx in enumerate(top_k_indices):\n",
    "#     predicted_token = tz.convert_ids_to_tokens([pred_idx])[0]\n",
    "#     token_weight = top_k_weights[i]\n",
    "#     print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1999,  1037, 26931,  1010,   103,  2632,  3597,  3726,  1045,\n",
       "         22404,  1012,  2009,  1005,  1055,  2026,  4920,  1011,  1999,  1011,\n",
       "          1996,  1011,  2813,  1010,  9996,  5507,  1012,  1045,  4530,  2116,\n",
       "          1037, 26035,  1010,  2664,  2026,  3125,  1005,  1055,  2000,  4372,\n",
       "         13149,  2026,  6203,  3129,  1010,  3183,  2023,  2518,  2089,  4906,\n",
       "          1012,   102]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tz.encode(text, return_tensors='pt')\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids_tk = tz.tokenize(text, return_tensors='pt')\n",
    "token_ids_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_position = (token_ids.squeeze() == tz.mask_token_id).nonzero()\n",
    "masked_pos = [mask.item() for mask in masked_position ]\n",
    "masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence :  In a cozy, [MASK] alcove I knit.\n",
      "It's my hole-in-the-wall, poorly lit.\n",
      "I drop many a stitch,\n",
      "Yet my goal's to enrich\n",
      "My dear husband, whom this thing may fit.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(token_ids)\n",
    "last_hidden_state = output[0].squeeze()\n",
    "print (\"sentence : \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c o z y', 's m a l l', 'w a r m', 'd a r k', 'c o m f o r t a b l e', 'l i t t l e', 'q u i e t', 'i n t i m a t e', 'n e a t', 't i n y', 'p r i v a t e', 'i n v i t i n g', 'l o n e l y', 's e c l u d e d', 'n a r r o w', 's h a d e d', 'c o o l', 'm o d e s t', 'd i m', 'p l e a s a n t', 'd u s t y', 'c r a m p e d', 't i d y', 'd a m p', 's h a d y', 's u n n y', 's o f t', 's e c r e t', 'l o v e l y', 'r u s t i c', 'e m p t y', 's m o k y', 'c l e a n', 'd i s c r e e t', 'r o m a n t i c', 'u n c o m f o r t a b l e', 'w e l c o m i n g', 'c o l d', 'n i c e', 'i s o l a t e d', 's i l e n t', 's p a c i o u s', 'w h i t e', 'c h e e r f u l', 'e l e g a n t', 'p l u s h', 'f e m i n i n e', 's h a d o w y', 'r o u n d', 'e n c l o s e d', 's h a d o w e d', 'o p e n', 's h a l l o w', 'i n f o r m a l', 'h u m b l e', 's a f e', 'b e a u t i f u l', 'l i t', 'c o m f o r t i n g', 'w o o d e n', 's i m p l e', 'h i d d e n', 'c i r c u l a r', 'c h i l l y', 'd r y', 'h u s h e d', 'o l d', 'c h a r m i n g', 'l a r g e', 'd e e p', 'c o r n e r', 'h e a t e d', 'g r e e n', 's e c u r e', 'm a s c u l i n e', 's h e l t e r e d', 'b l a c k', 'c o v e r e d', 'r e l a x i n g', 'f a m i l i a r', 'v e l v e t', 'h a p p y', 'l i g h t', 'b a r e', 's l e e p i n g', 'p i c t u r e s q u e', 'd a r k e n e d', 'l o w', 'p r e t t y', 's w e e t', 'f o r m a l', 'd i r t y', 'l u x u r i o u s', 'a n o n y m o u s', 's o l i t a r y', 'm o i s t', 's t e r i l e', 't i g h t', 'l a c y', 's c e n t e d']\n"
     ]
    }
   ],
   "source": [
    "list_of_list =[]\n",
    "\n",
    "for mask_index in masked_pos:\n",
    "    mask_hidden_state = last_hidden_state[mask_index]\n",
    "    idx = torch.topk(mask_hidden_state, k=100, dim=0)[1]\n",
    "    words = [tz.decode(i.item()).strip() for i in idx]\n",
    "    list_of_list.append(words)\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' c o z y'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_guess = \"\"\n",
    "for j in list_of_list:\n",
    "    best_guess = best_guess+\" \"+j[0]\n",
    "\n",
    "best_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentence\n",
    "encoded = tz.encode_plus(\n",
    "    text=lim,  # the sentence to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    max_length = 64,  # maximum length of a sentence\n",
    "    truncation=True,\n",
    "    padding='longest',  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('smartbard')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24ea05d60ccfa0ceea8ad1c3f4c58b3676276e6d2f47edf8677d2fc67d75419d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
