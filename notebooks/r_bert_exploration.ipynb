{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_processing.format_data import extract_ab_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../raw_data/limerick_dataset_oedilf_v3.json'\n",
    "df = pd.read_json(file_path)\n",
    "df = df[df.is_limerick == True]\n",
    "lines = extract_ab_lines(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Though indulgence has frequently showed'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_lines = lines[0]\n",
    "ex_line = a_lines[31]\n",
    "ex_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Though', 'in', '##du', '##lge', '##nce', 'has', 'frequently', 'showed']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz.tokenize(ex_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3343, 0.3567, 0.9100],\n",
      "        [0.7196, 0.4538, 0.8579],\n",
      "        [0.3113, 0.7047, 0.6230],\n",
      "        [0.8664, 0.7172, 0.8465],\n",
      "        [0.4166, 0.7034, 0.7823]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentence\n",
    "encoded = tz.encode_plus(\n",
    "    text=ex_line,  # the sentence to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    max_length = 64,  # maximum length of a sentence\n",
    "    truncation=True,\n",
    "    padding='longest',  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded['input_ids']\n",
    "attn_mask = encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  3473,  1107,  7641, 21463,  3633,  1144,  3933,  2799,   102]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 440M/440M [03:09<00:00, 2.33MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Though indulgence has frequently showed[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [SEP]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ex_line + \"[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\"\n",
    "text = \"[CLS] %s [SEP]\"%text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 376kB/s] \n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 10.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "tz = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tz.tokenize(text)\n",
    "masked_index = tokenized_text.index(\"[MASK]\")\n",
    "indexed_tokens = tz.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]: 'a'  | weights: 0.4407033324241638\n",
      "[MASK]: 'in'  | weights: 0.07142706960439682\n",
      "[MASK]: 'its'  | weights: 0.05816581845283508\n",
      "[MASK]: 'with'  | weights: 0.04569272696971893\n",
      "[MASK]: 'to'  | weights: 0.033848367631435394\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "top_k = 5\n",
    "top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "for i, pred_idx in enumerate(top_k_indices):\n",
    "    predicted_token = tz.convert_ids_to_tokens([pred_idx])[0]\n",
    "    token_weight = top_k_weights[i]\n",
    "    print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Though indulgence has frequently showed[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ex_line + \"[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2295, 27427,  5313, 17905,  2038,  4703,  3662,   103,   103,\n",
       "           103,   103,   103,   103,   103,   102]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tz.encode(text, return_tensors='pt')\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'return_tensors': 'pt'} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['though',\n",
       " 'ind',\n",
       " '##ul',\n",
       " '##gence',\n",
       " 'has',\n",
       " 'frequently',\n",
       " 'showed',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_tk = tz.tokenize(text, return_tensors='pt')\n",
    "token_ids_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_position = (token_ids.squeeze() == tz.mask_token_id).nonzero()\n",
    "masked_pos = [mask.item() for mask in masked_position ]\n",
    "masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence :  Though indulgence has frequently showed[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(token_ids)\n",
    "last_hidden_state = output[0].squeeze()\n",
    "print (\"sentence : \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'i n', 'i t s', 'w i t h', 't o', '\"', 'a s', 'n o', 't h e', 'a n', ',', 'o f', 'v e r y', \"'\", 'f o r', 'h i s', 'a t', 's o m e', 's u c h', 't h e i r', 'h a s', 'i s', 'i t', 'i t s e l f', 'b y', 'a n d', 'i n t o', 'h e r', 'r a t h e r', 'm u c h', 'a n y', 'q u i t e', ':', 't h i s', 'm o r e', 'o n l y', '.', '-', 'o n', 'h i m', 't h r o u g h', 'n o t', 'o r', 'o n e', 't h a t', 'e v e n', 'b u t', 'l i t t l e', 's o m e t h i n g', '(', 'a l m o s t', ')', '...', 'a l s o', 'f r o m', 'w a s', 's o m e t i m e s', 'a b o u t', 'm a n y', 't h e m', 'o f t e n', 'f a r', 'g e n e r a l l y', '# # l y', 'h a d', ';', 'b o t h', 't h e r e', 'w i t h o u t', 'e x t r e m e l y', 'i n c r e a s i n g l y', '# # s', 'r e l a t i v e l y', 'y e t', 't o o', 's o', 's o m e w h a t', 'o t h e r', 'h i m s e l f', 'a n o t h e r', 'p o s s i b l e', 'a g a i n s t', 't w o', 's h o w n', 'o u t', 's h o w i n g', 'h a v e', 's l i g h t l y', 'h a v i n g', 's e v e r a l', 'c l e a r', 'n e a r', 'h o w', 'r e l a t i v e', 'e x h i b i t s', ']', 'u n d e r', 's i g n i f i c a n t', 'm o s t', 'g r e a t']\n",
      "['a', 'i t s', 't o', 'i n', 'w i t h', '\"', 'a s', 't h e', 'o f', ',', 'n o', 't h e i r', \"'\", 'a n', 'a n d', 'h i s', 'f o r', 'a t', 'b y', 'v e r y', 's o m e', 'i s', 'o r', '# # s', 'h a s', '# # l y', 's o m e t i m e s', 'm o r e', 'i n t o', 'r a t h e r', 'b u t', 'h e r', 'o f t e n', 'n o t', '.', '-', 'i t', 't h i s', 'e v e n', '(', 'o n', 'a n y', 'm u c h', ':', 'a l s o', 't h a t', 's u c h', 'q u i t e', ')', 't h r o u g h', 'w a s', 'p o s s i b l e', '...', 'i t s e l f', 'f r o m', 'o n l y', 'a b o u t', 'p o t e n t i a l l y', 's t i l l', ';', 'y e t', 'h a v e', 'l i t t l e', 'o t h e r', 'h a v i n g', 'g e n e r a l l y', 's h o w i n g', 'a l m o s t', 'p e r h a p s', 'h a d', 't o w a r d', 'o n e', 's o m e t h i n g', 'b o t h', 'm a n y', 's o m e w h a t', 'i n c r e a s i n g l y', 't o w a r d s', 'a g a i n s t', 'm y', 'r e l a t i v e', 'r e l a t i v e l y', 'l e s s', 'c l e a r', 't o o', 'm o s t', 'f a r', 's l i g h t l y', 'c e r t a i n', 'e x t r e m e l y', 'a r e', 'u n d e r', 'n e a r', 'n o w', 'p o t e n t i a l', 'w i t h o u t', ']', 'a l w a y s', 'h o w', 'o c c a s i o n a l l y']\n",
      "['a n d', 't o', 'o r', 'o f', ',', '\"', 'i n', 'w i t h', 'i t s', '-', \"'\", 'f o r', 'a s', '# # s', 'b u t', 'i s', 'a', 't h e', '# # l y', 'o n', '(', '.', 'a t', 'h a s', 't h e i r', 'w a s', 'n o t', '/', 'b y', ';', 'a g a i n s t', 'i f', 'a r e', 's o m e t i m e s', 'i n t o', 's o c i a l', ')', 'a b o u t', 'w i t h o u t', 't h a t', 'o f t e n', ':', 'h i s', '...', '# # a l', 'u n d e r l y i n g', 'p o s s i b l e', 't o w a r d', 'a n', 'c a u s e s', 'f r o m', 'h u m a n', 'r e l a t i v e', 't h r o u g h', 'c a n', 'y e t', 't o w a r d s', 'p o l i t i c a l', 'n o', 't h a n', 'b e', '# # i a l', 'h a v e', 'm o r e', 'b e t w e e n', 'n o r', 'o t h e r', 'a l s o', 'h i s t o r i c a l', 'p o t e n t i a l', 'r e l a t i o n s h i p', '&', 'b e i n g', 'f o r m', 'i t', 'w h i l e', '# # i c', '# # a l l y', 'c a u s e', 'w e r e', 'u n d e r', 'c u l t u r a l', 'h a v i n g', 'c o m m o n', 'f i n a n c i a l', 'r a t h e r', '# # o l o g i c a l', 'h a d', 'm o r a l l y', 'a m o n g', 'p o t e n t i a l l y', 'w h e n', 't r u e', 'o v e r', 'm o r a l', 'l e s s', 'v e r s u s', 't h i s', '# # i n g', 'g r e a t e r']\n",
      "['a n d', 't o', 'o f', 'o r', 'w i t h', 'i t s', ',', 'i n', '-', '\"', 'i s', 'f o r', \"'\", 't h e', 'b u t', 'o n', '# # s', 'a s', '# # l y', 'a', 'w i t h o u t', 's o c i a l', '# # a l', '.', 'a g a i n s t', 't h a n', 'w a s', '(', 'a r e', 'h u m a n', 'h a s', 'i f', 'i n t o', '/', 'p o l i t i c a l', 'n o t', ';', 't h e i r', 'a b o u t', 'b y', '# # i a l', 't h a t', 'c u l t u r a l', '...', ')', 'c a n', 'a t', 'f r o m', 'r e l a t i v e', 'h i s t o r i c a l', '# # o l o g i c a l', 'c a u s e s', '# # i c', 'u n d e r l y i n g', 't o w a r d', 'h i s', ':', 't o w a r d s', 'v e r s u s', 'n o r', 't h r o u g h', 'p o s s i b l e', 'f i n a n c i a l', 's o m e t i m e s', 'e c o n o m i c', 'b e t w e e n', 'a n', 'o f t e n', 'm o r a l', '&', 'p u b l i c', 'y e t', 'c o m m o n', 'b e i n g', 'b e', '# # i n g', 'f o r m', 'a m o n g', 'o v e r', 'p a s t', 'r i s k', 'u p o n', 'p e r s o n a l', 'w e r e', 'i t', 'e m o t i o n a l', 'h a v e', '# # a b l e', 'l i t e r a r y', '# # l', 'r e l a t i o n s h i p', 's c i e n t i f i c', 'm o r a l l y', 'n o', 'w h i l e', 'a r t i s t i c', 'p s y c h o l o g i c a l', 'w i t h i n', 's o c i a l l y', 'w h e n']\n",
      "['t o', 'a n d', 'o f', 'i s', 'w i t h', 'f o r', 'i n', 'o r', '# # s', '# # l y', '\"', 'i t s', '-', '# # a l', ',', 'o n', 'a s', \"'\", 'w a s', 'a r e', 't h a n', 't h e', '.', 'a g a i n s t', 'b u t', 'a', 'c a n', 'i n t o', 't h a t', 'b e t w e e n', ')', 'i f', 'a t', '(', 'h a s', '# # i c', 'a b o u t', 't o w a r d s', 't o w a r d', '/', 'n o t', 'f r o m', '# # i t y', 'w h e n', ':', 'r e s u l t s', 'b e i n g', 's o c i a l', 'i t', 'v a l u e', ';', 'r e l a t i o n s h i p', 'w o u l d', '# # i n g', '# # o l o g i c a l', 'w e r e', '# # e d', '# # y', 'w i t h o u t', '# # l', 'b y', '# # a b l e', 'v a l i d i t y', '# # n e s s', '# # i a l', '# # b i l i t y', 'b e', 'c o u l d', 'c u l t u r a l', 't h e i r', '# # i v e', '# # i s m', 'h i s', 's o m e t i m e s', 'f o r m', '# # t', 'c o s t', 'e x p r e s s i o n', 'w i l l', 'o f t e n', 'r i s k', 'c a u s e s', 'w i t h i n', '# # d', '# # e', '# # a l l y', 'o v e r', '# # m a t i c', 'a f t e r', 'v s', 'f o l l o w i n g', 'c o n t r o l', 'p e r f o r m a n c e', '# # i', '...', '# # i s t i c', 's t', 'u p o n', '# # u s', 'u n d e r l y i n g']\n",
      "['# # s', '\"', 't o', '.', ')', \"'\", 'e x i s t s', '# # i s m', ',', 't o d a y', 'r e s u l t s', 'o f', 't h e r e o f', 'h e r e', 'v a l u e', 'i n', 't h e r e', 'i s', 'f o r', '# # l y', 'o c c u r s', '# # a b i l i t y', 'o b s e r v e d', 'w i t h i n', 'b e l o w', ';', ':', '-', 'o c c u r', 'a l s o', 'f o l l o w e d', 'e f f e c t s', 'c o m p a r e d', 't h r o u g h o u t', 's o m e t i m e s', 'o f t e n', 'r a t e s', 'a b o v e', 'o c c u r r e d', 'h o w e v e r', 'e x i s t', 'b e t w e e n', 's i n c e', 'a r e', 'a s', 'v a l u e s', 't h a n', 'r a t e', 'i t s e l f', '# # n e s s', 'i t', 'p e r f o r m a n c e', '# # y', 'a n d', 'g e n e r a l l y', 'i s s u e s', 'c o n t r o l', 'c a n', 't h a t', 'e l s e w h e r e', 'r e p o r t e d', 'p r e s e n t', 'l a t e r', '# # i n g', 'o n', 'e f f e c t', '# # i t y', '# # t', '# # e s', 'i n v o l v e d', 'p o s s i b l e', 'n o w', 'f o r m', 'b e h a v i o r', 'r e s u l t', 'p o i n t s', 'h a s', 'e v i d e n c e', '# # l', 'a f t e r w a r d s', 'c o n s e q u e n c e s', 'g o a l s', '# # a l', 'i n t e r e s t', 'r e s u l t e d', 's h o w n', 'a n a l y s i s', 'w h a t s o e v e r', 'b o t h', 'b e n e f i t s', 'f o l l o w s', 'w a s', '# # m', 'o c c u r r i n g', 'f o l l o w i n g', 'b e h i n d', 'w i t h', 'r e l a t i o n s h i p', 'c l a i m s', '# # i v i t y']\n",
      "['.', ';', '!', '|', '?', '\"', \"'\", ')', ':', '...', ',', '-', ']', '।', '}', '(', 'o f', '॥', 'i n', '[ U N K ]', 't o', 'i s', 'f o r', 'o n', '# # ¦', '*', 'f r o m', '#', 'w i t h i n', '# # s', 'a r e', '# # ¤', 'a n d', '/', '~', 'w e b s i t e', 'a', 'd e f a u l t e d', 't o d a y', '%', 'b y', '>', 'i t', 'y o u t u b e', 'n o n e', 'o n e', 'b e l o w', 't h e', 'a s', '。', 's', 'h e r e', '．', 'i n c l u d e', '# # a', '=', 'i t s e l f', 'w a s', 'o f f', 't h a t', 'o n l i n e', 'i n s t r u m e n t s', 's o u n d', 'w i t h', '$', 'a b o u t', '§', 'w h e n', 'b i l l b o a r d', 'e x i s t s', 's o u r c e', 'c a n', '[', 'r i s k', 't h a n', 'd a t a', 'p e r f o r m a n c e', 'i n v o l v e d', 'c a u s e', '+', 'u n d e r', '1', 'a l b u m', 'n o t', 'i n t o', 's o', 'r e c o r d s', 't h r o u g h', 'r e c o r d', 'h e', 't h e r e', 'b e c a u s e', 'b e', 'c h o r u s', 'e l s e w h e r e', 'p h o t o g r a p h y', 'r e s u l t s', 'p u b l i c', 'e x i s t', 't h e r e o f']\n"
     ]
    }
   ],
   "source": [
    "list_of_list =[]\n",
    "\n",
    "for mask_index in masked_pos:\n",
    "    mask_hidden_state = last_hidden_state[mask_index]\n",
    "    idx = torch.topk(mask_hidden_state, k=100, dim=0)[1]\n",
    "    words = [tz.decode(i.item()).strip() for i in idx]\n",
    "    list_of_list.append(words)\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a a a n d a n d t o # # s .'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_guess = \"\"\n",
    "for j in list_of_list:\n",
    "    best_guess = best_guess+\" \"+j[0]\n",
    "\n",
    "best_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('smartbard')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24ea05d60ccfa0ceea8ad1c3f4c58b3676276e6d2f47edf8677d2fc67d75419d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
